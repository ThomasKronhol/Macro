---
title: "Macroeconometrics: An Investigation of the Effects of Financial Conditions on the US Real Economy"
author: 
  - name: "Thomas Kronholm Moeller"
# format:
#   pdf:
#     fontfamily: cmbright
#     geometry: margin=1.5cm
format:
  html:
    toc: true
    toc-location: left
editor: 
  markdown: 
    wrap: 72
---

> Disclaimer: This page is an ongoing research project conducted as a
> part of Macroeconometrics (ECOM9007) at The University of Melbourne,
> Australia.\

## Research question & Motivation

As the financial crisis emerged in 2008, the world economy realized just
how much financial conditions affected the real economy. This research
project intends to quantify the effects of a tightening of the financial
conditions on the real economy.

The objective question to be answered can thus be summarized as follows;
Does a tightening of the financial conditions have the anticipated
effects on the real economy, and if so, what are the magnitudes?

**What motivates this?**

It is, or rather has been a standard part of macroeconomic modelling to
exclude the financial sector from applied theoretical modelling. This
has been a popular choice of researchers, who have argued that there is
no or at least a negligible effect of financial variables on real
variables. Nevertheless, financial crises are often followed by
significant drops in consumption and production, as evident from the
figure of the respective series in the forthcoming section. This might
be a result of lower consumer sentiment, which possibly through a wealth
channel, affects the overall demand for goods as well as savings. Thus,
getting a thorough understanding of the effect of the financial
conditions on the real economy is of importance for policy makers, who
should take these insights into account when tightening the financial
conditions through tighter monetary and/or macroprudential stances, as
they might result in some undesired outcomes.

Furthermore, the research question is highly applicable in today's
economic climate. As central banks have started raising rates world
wide, the financial condition index enables us to get a complete view of
the effects that this might have by looking at financial markets, credit
and liquidity while including the shadow banking system. The analysis is
somewhat inspired by @JensenandRoager2019, who finds that easing
financial conditions had a positive impact on the Danish economy using
quarterly data.

## Data and its properties

As mentioned in the prior section the results of the structural analysis
should be of immediate use for economic policy makers, and thus a high
frequency is of importance. The model will therefore be estimated using
monthly data for the economy of the United States.

The economic variables used for the empirical analysis are given by:

The economic activity, and thus a measure of the real economy, is
modeled by **industrial production** which is an approximate variable
for the movements in Gross Domestic Product (GDP). Industrial production
is often used as a measure for real economic activity, and is in
particular considered sufficient for economies with a large
manufacturing sector. This implies that later on, the response of the
variable can be interpreted as the effect of changes in NFCI to the real
activity.

Additionally, the **consumer price index (CPI)** has been introduced to
the model as well. The price level is included as financial conditions
might affect the price level. Furthermore, given the mandates of the
Federal Reserve (Fed), which includes keeping inflation steady while
maintaining a high rate of employment, it is of importance getting an
understanding of the relation between these variables.

In order to directly link the movements in the financial condition index
and the real economy, total **lending** from commercial banks is
applied. This is due to the fact that as financial conditions are
possibly tightened through monetary or financial regulatory authorities,
it might become harder to obtain a loan for households as well as firms,
which might impact the economic activity, as evident from the financial
crisis in 2008.

Additionally, as it is common to have a large amount of your wealth in
real estate, **S&P U.S. National Home Price Index** has been included.
The behavior of the real estate prices might especially have an impact
in consumer sentiment possibly through the aforementioned wealth
channel, which could then affect the real economy through lower demand
for goods or a higher demand for savings. The model therefore includes
**consumer sentiment** provided by University of Michigan. The variables
of consumer sentiment will be denoted consumer expectations throughout
the analysis. In order to take into account the monetary policy
response, the **federeal funds rate** has been included.

Lastly, as we are interested in identifying a shock to financial
conditions, the **National Financial Condition Index (NFCI)** is
included. The index is constructed by the Federal Reserve, Chicago and
is a measure of the conditions of finance, taking financial markets,
credit and liquidity and the shadow banking system into account.

The time series are retrieved using Fred, the Economic database provided
by the Federal Reserve Bank og St.Louis. The time period considered is
from 01.01.1987 - 01.01.2023. The data is obtained using package
**fredr()**.

```{r Getting data from fred}
#| echo: false
#| message: false
#| warning: false
#| results: hide

library(fredr)
library(ggplot2)
library(dplyr)
library(purrr)
library(vars)
library(xts)
library(gridExtra)
library(tseries)
library(tidyverse)
library(palmerpenguins)
library(quarto)
library(tinytex)
library(quantmod)
library(mvtnorm)
library(MASS)
library(HDInterval)
library(parallel)

#Setting up API key from FRED
fredr_set_key("54706a95d44824ac499f1012d9b3a401")

start_date <- as.Date("1987-01-01")
end_date <- as.Date("2023-01-01")

# Define a vector of series IDs
timeseries <- c("INDPRO", "CPIAUCSL","FEDFUNDS", "UMCSENT", "TOTCI", "CSUSHPISA", "NFCI")
data <- list()

# Retrieve the data for the series and date range ::::: change m to q for quarterly
for (name in timeseries) {
  fred_data <- fredr(series_id = name, observation_start = start_date, observation_end = end_date, frequency = "m")
  data[[name]] <- fred_data
}

#Deleting columns not required 
indp <- data[["INDPRO"]]
indp$ln_value <- log(indp$value)
indp_ <- indp[, c("date","ln_value")]
indp_$date <- as.Date(indp_$date, format = "%d-%m-%Y")
tindp <- xts(indp_$ln_value, order.by = indp_$date)

cpi <- data[["CPIAUCSL"]]
cpi$ln_value <- log(cpi$value)
cpi_ <- cpi[, c("date","ln_value")] 
tcpi <- xts(cpi_$ln_value, order.by = indp_$date)

FF <- data[["FEDFUNDS"]]
FF_ <- FF[, c("date","value")]
tFF <- xts(FF_$value, order.by = indp_$date)

expec <- data[["UMCSENT"]]
expec_ <- expec[, c("date","value")]
texpec <- xts(expec_$value, order.by = indp_$date)

lend <- data[["TOTCI"]]
lend$ln_value <- log(lend$value)
lend_ <- lend[, c("date","ln_value")]
tlend <- xts(lend_$ln_value, order.by = indp_$date)

hp <- data[["CSUSHPISA"]]
hp$ln_value <- log(hp$value)
hp_ <- hp[, c("date","ln_value")]
thp <- xts(hp_$ln_value, order.by = indp_$date)

fci <- data[["NFCI"]]
fci_ <- fci[, c("date","value")]
tfci <- xts(fci_$value, order.by = indp_$date)

#Merging the series into vector Y
Y = na.omit(merge(tindp, tcpi, tFF, texpec, tlend, thp, tfci))
colnames(Y)<- c("IP", "CPI","FF", "EXP", "LEND","HP","NFCI")

```

```{r plotting the series}
#| echo: false
#| message: false
#| warning: false
#| results: hide
#Plotting the series in levels
ip = ggplot(data = Y[,1], aes(x = index(Y[,1]), y = Y[,1])) +
  geom_line(color = "black") +
  labs(title = "Industrial production", x = "Year", y = "Ln(Indp)") +
  theme_minimal()

inf = ggplot(data = Y[,2], aes(x = index(Y[,2]), y = Y[,2])) +
  geom_line(color = "black") +
  labs(title = "Consumer price index", x = "Year", y = "Ln(CPI)") +
  theme_minimal()

FF = ggplot(data = Y[,3], aes(x = index(Y[,3]), y = Y[,3])) +
  geom_line(color = "black") +
  labs(title = "Federal Funds Rate", x = "Year", y = "Percent") +
  theme_minimal()

ex = ggplot(data = Y[,4], aes(x = index(Y[,4]), y = Y[,4])) +
  geom_line(color = "black") +
  labs(title = "Consumer Expectations", x = "Year", y = "Index") +
  theme_minimal()
    
le = ggplot(data = Y[,5], aes(x = index(Y[,5]), y = Y[,5])) +
  geom_line(color = "black") +
  labs(title = "Total Lending", x = "Year", y = "Ln(Lending)") +
  theme_minimal()

hou = ggplot(data = Y[,6], aes(x = index(Y[,6]), y = Y[,6])) +
  geom_line(color = "black") +
  labs(title = "House price index", x = "Year", y = "Ln(HP)") +
  theme_minimal()

fincon = ggplot(data = Y[,7], aes(x = index(Y[,7]), y = Y[,7])) +
  geom_line(color = "black") +
  labs(title = "National Financial Condition index", x = "Year", y = "Index") +
  theme_minimal()

```

**Preliminary data analysis**

The six time series are presented in the figure below. All variables,
except the two indexes, are transformed using the logarithm.

The industrial production, consumer price index, the house price index
and overall lending seems to follow an upward trend. Nevertheless,
significant events such as the great financial crisis of 2008 and the
outbreak of Covid-19 have had significant impact on the short term
movements in the respective series. Looking closer at the two indices,
they seem to be somewhat negatively correlated, indicating that the
aforementioned hypothesis of NFCI affecting consumer sentiment might be
somewhat visually present.

```{r showing plot}
#| echo: false
#| #| message: false
#| warning: false
#| label: fig-series-plot
#| fig-cap: "Time Series Plots"
grid.arrange(ip, inf,FF, ex, le, hou, fincon, nrow = 4, ncol = 2)

```

In order to get a deeper understanding of the order of integration of
the time series, the Autocorrelation function has been plotted in the
graph below. The plot indicates, that the series are highly
autocorrelated, thus indicating a univariate parameter value close to
unity implying a high degree of memory.

```{r showing acf plot}
#| echo: false
#| message: false
#| warning: false
# Plotting Autocorrelation functions
 Y = as.ts(Y)
# par(mfrow=c(4,2))
# a_ip = acf(Y[,1], lag.max = 12, main = "ACF Plot, Industrial production", ylab = "Autocorrelation", type = "correlation")
# a_cpi = acf(Y[,2], lag.max = 12, main = "ACF Plot, Consumer price index", ylab = "Autocorrelation", type = "correlation")
# a_FF = acf(Y[,3], lag.max = 12, main = "ACF Plot, Federal Funds Rate", ylab = "Autocorrelation", type = "correlation")
# a_exp = acf(Y[,4], lag.max = 12, main = "ACF Plot, Consumer expectations", ylab = "Autocorrelation", type = "correlation")
# a_le = acf(Y[,5], lag.max = 12, main = "ACF Plot, Total Lending", ylab = "Autocorrelation", type = "correlation")
# a_hou = acf(Y[,6], lag.max = 12, main = "ACF Plot, House price index", ylab = "Autocorrelation", type = "correlation")
# a_fci = acf(Y[,7], lag.max = 12, main = "ACF Plot, NFCI", ylab = "Autocorrelation", type = "correlation")

```

```{r ACF}
#| echo: false
#| message: false
#| warning: false
#| label: fig-acf-plot
#| fig-cap: "ACF Plots"
names = c("IP", "CPI","FF", "EXP", "LEND","HP","NFCI")
par(mfrow=c(4,2), mar=c(2,2,2,2))
for (j in 1:length(colnames(Y))){
  acf(Y[,j],main="")
  title(main = paste(names[j]), line = 1)
}
```

```{r PACF}
#| echo: false
#| message: false
#| warning: false
#| label: fig-pacf-plot
#| fig-cap: "PACF Plots"
par(mfrow=c(4,2), mar=c(2,2,2,2))
for (j in 1:length(colnames(Y))){
  pacf(Y[,j],main="")
  title(main = paste(names[j]), line = 1)
}
```

In order to examine the order of integration, an Augmented Dicky Fuller
test is conducted using function **adf()**. The test statistically tests
for the existence of a unit root in the time series univariatly. The lag
length used for the test is chosen to be 12. This is primarily a result
of the data being monthly. The results can be found in the table below.

```{r test}
#| echo: false
#| message: false
#| warning: false
#| results: hide
#Doing an ACF test :::::::: change to 6 if HP incl.
#
# max_lag = 12
# adf_ <- list()
# for (i in 1:6) {
#   adf_result = adf.test(Y[,i], k = max_lag)
#   adf_[[i]] <- adf_result
# }
# head(adf_)
# 
# # View the ADF test results
# summary(adf_result)
# 
# adf_table <- data.frame(Test_Statistic = numeric(length(adf_)), 
#                         p_value = numeric(length(adf_)), 
#                         Lags_Used = numeric(length(adf_)))
# 
# # Fill in the data frame with the test results
# for (i in 1:length(adf_)) {
#   adf_table[i, "Test_Statistic"] = round(adf_[[i]]$statistic,3)
#   adf_table[i, "p_value"] = round(adf_[[i]]$p.value,3)
#   adf_table[i, "Lags_Used"] = round(adf_[[i]]$parameter,3)
# }

```

```{r adftest}
#| echo: false
#| message: false
#| warning: false
#| results: hide

#ADF test for both levels and first diff

max_lag = 12
adf_ <- list()
adf_diff <- list()
for (i in 1:7) {
  adf_result = adf.test(Y[,i], k = max_lag)
  adf_[[i]] <- adf_result
  adf_diff_result = adf.test(diff(Y[,i]), k = max_lag) 
  adf_diff[[i]] <- adf_diff_result
}

#set up tables
adf_table_levels <- data.frame(Test_Statistic = numeric(length(adf_)), 
                        p_value = numeric(length(adf_)), 
                        Lags_Used = numeric(length(adf_)))

adf_table_diff <- data.frame(Test_Statistic = numeric(length(adf_diff)), 
                        p_value = numeric(length(adf_diff)), 
                        Lags_Used = numeric(length(adf_diff)))

for (i in 1:length(adf_)) {
  adf_table_levels[i, "Test_Statistic"] = round(adf_[[i]]$statistic,3)
  adf_table_levels[i, "p_value"] = round(adf_[[i]]$p.value,3)
  adf_table_levels[i, "Lags_Used"] = round(adf_[[i]]$parameter,3)
}

for (i in 1:length(adf_diff)) {
  adf_table_diff[i, "Test_Statistic"] = round(adf_diff[[i]]$statistic,3)
  adf_table_diff[i, "p_value"] = round(adf_diff[[i]]$p.value,3)
  adf_table_diff[i, "Lags_Used"] = round(adf_diff[[i]]$parameter,3)
}

# Compute tables
rownames(adf_table_levels) <- c("IP", "CPI", "FF", "EXP","LEND","HP","NFCI")
colnames(adf_table_levels)<- c("Test statistic", "P-value", "Lags")
knitr::kable(adf_table_levels, digits = 3)

rownames(adf_table_diff) <- c("\u0394IP", "\u0394CPI", "\u0394FF", "\u0394EXP","\u0394LEND","\u0394HP","\u0394NFCI")
colnames(adf_table_diff)<- c("Test statistic", "P-value", "Lags")
knitr::kable(adf_table_diff, digits = 3)

```

```{r test show}
#| echo: false
#| message: false
#| warning: false

# Print the data frame
# rownames(adf_table)<- c("Industrial Production", "Consumer Price Index", "Consumer Expectations", "Total Lending","House price index","NFCI")
# colnames(adf_table)<- c("Test statistic", "P-value", "Lags")
# print(adf_table)
#| echo: false
#| message: false
#| warning: false
knitr::kable(adf_table_levels, digits = 3)

```

As evident from the table we are not able to reject the null hypothesis
of the presence of a unit root in any of the time series at a 5 pct.
level of significance, and thus not able to reject the hypothesis of the
variables being integrated of order 1. Now, taking the first-difference
of the series and checking for the possibility of the series being
integrated of order 2 we see, that it is rejected on a 5 pct. level of
significance.

```{r test show diff}
#| echo: false
#| message: false
#| warning: false

#| echo: false
#| message: false
#| warning: false
knitr::kable(adf_table_diff, digits = 3)

# Testing for cointegration rank with trend
#vecm_Y = ca.jo(Y, type = "trace", ecdet = "trend", K = 5, spec = "transitory")
#summary(vecm_Y) 

```

The order of integration is of particular interest when doing structural
analysis, given that the shocks to stationary processes can be
considered temporary, while shocks to I(1)-processes can be considered
permanent given that random walk processes has a high degree of memory
from past shocks. Thus, as the variables are I(1)-processes, all shocks
can be considered permanent. Given the properties of the time series
multiple priors are available. First of all, following the literature,
using a Normal-Wishart prior could be sufficient in getting some
reliable estimates of the shocks. However, one could also use a
dummy-prior, enabling the econometrician to examine the joint dynamics
of the time series aforementioned in the long run. This could be done by
following the work of @giannone2019priors.

## Econometric model and hypothesis

In order to determine the effect of a tightening of the financial
conditions to the real economy one could apply a structural vector
autoregressive model (SVAR), which enables an identification of that
exact shock. A general version of the SVAR with q-lags is presented
below: \begin{gather}
        B_0y_t=c_0+B_1y_{t-1}+B_2y_{t-2}+...+B_qy_{t-q}+\varepsilon_t,
\end{gather} where $y_t$ is a $K \times 1$ matrix containing the
variables outlined in section "Data and its properties", $B_i$ is a
$K \times K$ and $c$ and $\varepsilon_t$ are $K \times 1$ matrices where
$K= \text{number of variables}$. The $B_0$ is known as the structural
matrix, containing contemporaneous relationships. $\varepsilon_t$
conditionally on $Y_{t-1}$ contains the orthogonal shocks with
$\varepsilon_t \sim iid(0_K,I_K)$.

For convenience researchers often consider the reduced form of the
structural model. Pre-multiplying the model with $B_0^{-1}$, rotating
the model from the structural form to the reduced form. The model can be
written as:\
\begin{gather} \label{svar}
    y_t=\mu+A_1y_{t-1}+A_2y_{t-2}+...+A_qy_{t-q}+u_t,
\end{gather} where $A_j=B^{-1}_0B_j$, $\mu=B_0^{-1}c_0$ and
$u_t=B^{-1}_0\varepsilon_t$ and where $u_t|Y_{t-1}\sim iid(0_K,\Sigma)$,
where $\Sigma=B^{-1}_0B^{-1'}_0$.

The structure of $B_0^{-1}$ can be imposed in numerous ways, although in
order for the model to be identified using exclusion it must be the
case, that we impose $K(K-1)/2$ restrictions. In this research paper a
cholesky decompostion will be applied. This implies a recursive
identification which imposes the $B_0^{-1}$ to be lower triangular. In
general the ordering of the variables in $y_t$ must be justified using
economic theory. Nevertheless this research paper follows the ordering
introduced in @JensenandRoager2019, where the variables are ordered from
slow to fast moving, thus financial variables will be ordered in the
last, while real output and inflation will be ordered first. This might
be economically justified by the fact that, even though the variables
are all monthly, the impact of the financial variables to the real
economy might take one period, thus no contemporaneous effects can be
expected.

**How to use the structural model and proposed output**

Having estimated the structural model, one could correctly examine the
effects to the real economy of a shock to the financial conditions.
Using a shock of one standard deviation, the structural impulse response
functions (IRF) can be computed. The impulse responses indicate how the
real economy responds to a tightening of the financial conditions.
Having correctly imposed the recursive scheme on the model introduced in
the prior section would enable us to see if there is a significant
response on the real variables and additionally if the causality
assumption of the consumer expectations, and its effect onto economic
variables seem justified statistically.

Relating the signs of the impulse responses to the aforementioned
hypothesis, one would theoretically expect that tighter financial
conditions affected the consumer expectations negatively, which would
have negative spill-overs to demand and thus production.

In order to get a better understanding of how much financial conditions
affect the variables of interest one could compute a forecast error
variance decomposition, and given that there at some point in the
observed period might have been some kind of paradigm shift, using a
historical decomposition can be used to see if the explanatory power of
the financial conditions onto economic variables have changed over time.

## Estimation procedure and model extensions

In order to estimate the model the model outlined in the previous
section I follow the algorithm proposed by @waggoner2003gibbs.

## Basic Model

we start by rewriting the structural model proposed proposed in the
former section. Using that
$B_+ = \begin{bmatrix} c_0 & B_1 & \dots & B_q \end{bmatrix}$ and
$x_t = \begin{pmatrix} 1 & y'_{t-1} & \dots & y'_{t-q} \end{pmatrix}'$
we have that the model can be written as follows: \begin{gather}
B_0y_t= B_+x_t + u_t, \text{ where } u_t \sim N(0,1)
\end{gather}

$B_0$ is the structural matrix containing the exclusionary restrictions.
By using that $B_{0[n\cdot]}=b_n\;V_n$ , where $b_n$ is a vector of
unrestricted elements and $V_n$ is a matrix consisting of only ones and
zeroes, which ensures that the restrictions are imposed on the right
elements. The dimension of $b_n$ and $V_n$ is $1\times r_n$ and
$r_n\times N$ respectively. This implies that the restrictions will be
implemented on each row of the structural matrix such that
$B_0=\begin{bmatrix} b_1V_1 & \dots & b_NV_N \end{bmatrix}'$. Using the
arguments the structural model can be written as: \begin{align}
b_nV_nY &= B_nX+ U_n\\
U_n   &\sim \mathcal{N}(0_T,I_T)
\end{align} The dimensions of the matrices are given by; $Y$ is a
$N\times T$ matrix, $X$ is a $K\times T$, $U_n$ is a $1\times T$ matrix
and $B_n=B_{+[n\cdot]}$ is of dimension $1\times K$.

In order to derive the posterior distribution, the likelihood function
of $B_0$ and $B_+$ given data as a $\mathcal{NGN}$ distribution is
introduced and given by:

```{=tex}
\begin{align}
L(B_+,B_0 | Y, X) \propto |\det(B_0)|^T \exp \left\{-\frac{1}{2} \sum_{n=1}^N (b_nV_nY-B_nX)(b_nV_nY-B_nX)'  \right\}
\end{align}
```
Now moving on the to the natural-conjugate prior, we know from
@waggoner2003gibbs, that this can be represented by a
normal-generalized-normal-distribution:
$p(B_+,B_0)\sim \mathcal{NGN}(\underline{B}, \underline{\Omega}, \underline{S}, \underline{\nu})$,
where:

```{=tex}
\begin{align}
p(B_+,B_0)&=\left(\prod_{n=1}^N p(B_n|b_n)\right)p(b_1,\dots,b_n)\\
p(B_n|b_n)&\sim \mathcal{N}_K (b_nV_n\underline{B},\underline{\Omega})\\
p(b_1,\dots,b_n) &\propto |\det (B_0)|^{\underline{\nu}-N} \exp \left\{-\frac{1}{2}\sum_{n=1}^Nb_nV_n\underline{S}^{-1}V_n'b_n'\right\}
\end{align}
```
Thus using the likelihood function and the naturcal-conjugate prior we
can state the kernel of the natural-conjugate prior distribution given
by:

```{=tex}
\begin{align}
|\det(B_0)|^{\underline{\nu}-N} \exp \left\{-\frac{1}{2} \sum_{n=1}^N b_nV_n\underline{S}^{-1}V_n'B_n'\right\} \times \exp \left\{-\frac{1}{2} \sum_{n=1}^N (B_n-b_nV_n\underline{B})\underline{\Omega}^{-1}(B_n-b_nV_n\underline{B})'\right\}
\end{align}
```
The prior parameters to be exploited is given by:

```{=tex}
\begin{align}
\underline{B} &= \left[0_{N\times 1}\;I_N\;0_{N\times(p-1)N}\right]\\
\underline{\Omega} &= \text{diag} \left(\left[\kappa_2\;\kappa_1(\textbf{p}^{-2}\otimes I_N')\right)\right]\\
\underline{S} &= \kappa_0I_N\\
\underline{\nu} &= N
\end{align}
```
This enables us to derive the posterior distribution using the kernel
outlined:

$$
\begin{align}
p(B_+,B_0|Y,X) &\propto L(B_+,B_0|Y,X)p(B_+,B_0)\\
               &\propto |\det(B_0)|^T \exp \left\{-\frac{1}{2} \sum_{n=1}^N (b_nV_nY-B_nX)(b_nV_nY-B_nX)'  \right\}\\
               &\times |\det(B_0)|^{\underline{\nu}-N} \exp \left\{-\frac{1}{2} \sum_{n=1}^N b_nV_n\underline{S}^{-1}V_n'B_n'\right\} \\ &\times \exp \left\{-\frac{1}{2} \sum_{n=1}^N (B_n-b_nV_n\underline{B})\underline{\Omega}^{-1}(B_n-b_nV_n\underline{B})'\right\}
\end{align}
$$

Completing the squares gives us the following expression

```{=tex}
\begin{align}
p(B_+,B_0|Y,X) &\propto |\det(B_0)|^{T+\underline{\nu}-N} \exp \left\{-\frac{1}{2} \sum_{n=1}^N (B_n-b_nV_n\bar{B})\bar{\Omega}^{-1}(B_n-b_nV_n\bar{B})'+b_nV_n\bar{S}^{-1}V_n'b_n'\right\}
\end{align}
```
Where the posterior distribution is then given by:

```{=tex}
\begin{align}
p(B_+,B_0|Y,X) &\sim \mathcal{NGN}(\bar{B},\bar{\Omega},\bar{S},\bar{\nu})\\
\bar{\Omega}&=\left[XX'+\underline{\Omega}^{-1}\right]^{-1}\\
\bar{B}&=\left[YX'+\underline{B\Omega}^{-1}\right]\bar{\Omega}\\
\bar{S}&=\left[YY'+\underline{S}^{-1}+\underline{B\Omega}^{-1}\underline{B}'-\bar{B}\bar{\Omega}^{-1}\bar{B}'\right]^{-1}\\
\bar{\nu}&= T+\underline{\nu}
\end{align}
```
Having formally stated the kernel of the basic model an outline of the
Gibbs sampler can be provided.

## Gibbs sampler and normalization

Given the Natural-conjugate prior distribution as already outlined above
the sampler for the contemporaneous relations ship matrix $B_0$ is drawn
row-by-row from the full conditional distributions given by:
\begin{gather*}
    p(b_n | Y, X, b_1, \dots, b_{n-1}, b_{n+1}, \dots, b_N) 
\end{gather*} Starting from this, the posterior sample
$\{b_1^{(s)},\dots, b_N^{(s)}\}^{S}_{s=1}$ can be computed.

The gibbs sampler for
$b_n^{(s)} \sim p(b_n | Y, X, b_1, \dots, b_{n-1}, b_{n+1}, \dots, b_N)$
is computed by following the algorithm proposed by @waggoner2003gibbs:

-   $U_n = \text{chol}\Big(\bar{\nu}\Big(V_n\bar{S}^{-1}V_n'\Big)^{-1}\Big)$
    where $U_n$ is a $r_n\times r_n$ matrix, with $r_n$ being the
    $n^{th}$ row.

<!-- -->

-   $w = [B_{0[-n.]}^{(s)}]$ where $w$ is a $1 \times N$ matrix

-   $w_1 = wV_n'U_n'\cdot \Big( wV_n'U_n'V_nU_nw'\Big)^{\frac{1}{2}}$
    where $w_1$ is a $1 \times r_n$ vector

-   $W_n=\begin{pmatrix} w_1' & w_{1\perp}' \end{pmatrix}$ where $W_n$
    is a matrix of dimensions $r_n \times r_n$

We now construct the matrix $\underset{1 \times r_n}{\alpha_n}$. This is
done by drawing the first element of the matrix starting with:

-   $u \sim N(0_{\nu+1},{\bar{\nu}^{-1}I_{\nu+1}})$

-   Additionally setting
    $\alpha_{n[\cdot 1]} = \begin{cases}\sqrt{u'u} \text{ with probability 0.5}\\-\sqrt{u'u} \text{ with probability 0.5}\end{cases}$

The remaining $r_n-1$ elements of $\alpha_n$ can be drawn from
$N(0_{r_n-1},\bar{\nu}^{-1}I_{r_n-1})$, after which the draw of the full
conditional distribution of $b_n$ can be computed by
$b_n^{(s)}\alpha_nW_nU_n$.

Having computed the posterior sample, we must now normalize the sample
as this ensures that we have found a unique maximum. The normalization
is done by considering a normalisation of each draw from the posterior
distribution of $B_0^{(s)}$. Introducing a set of diagonal normalizing
matrices $\underset{N\times N}{Q_i}, i\in 1, \dots, 2^N$, with diagonal
elements set to either 1 or -1, the distance between $Q_iB_0^{(s)}$ and
$\hat{B_0}$, where the latter term is the estimated matrix of the
contemporaneous effects can be derived. The distance is given by:
\begin{equation*}
    d \Big[Q_i B_{0}^{(s)}-\hat{B_{0}}^{-1'} | (\hat{B_0}' \hat{B_{0}})^{-1}\Big]
\end{equation*} Having found the $i$ minimizing the distance,
$Q_{i*}B_0^{(s)}$, we can apply direct sampling determining $B_+$ from
its multivariate normal distribution, drawn for each $b_n^{(s)}$.

## Algorithms and functions

The functions below are provided by Tomasz Wozniak and are necessary to
use the Gibbs sampler to replicate the algorithm provided by
@waggoner2003gibbs:

The first function introduced computes an orthogonal complement matrix
to X which is used in the following rgn-function.

```{r firstfunc}
#| echo: true
#| message: false
#| warning: false

orthogonal.complement.matrix.TW = function(x){
  N     = dim(x)
  tmp   = qr.Q(qr(x, tol = 1e-10),complete=TRUE)
  out   = as.matrix(tmp[,(N[2]+1):N[1]])
  return(out)
}
```

The function **rgn()** simulates draws for the unrestricted elements of
the contemporaneous relationships matrix of the structural model from a
generalized normal distribution

```{r secfunc}
#| echo: true
#| message: false
#| warning: false
rgn             = function(n,S.inv,nu,V,B0.initial){
  # n     - a positive integer, the number of draws to be sampled
  # S     - an NxN positive definite matrix, a parameter of the generalized-normal distribution
  # nu    - a positive scalar, degrees of freedom parameter
  # V     - an N-element list, with fixed matrices
  # B0.initial - an NxN matrix, of initial values of the parameters
  N             = nrow(B0.initial)
  no.draws      = n
  
  B0            = array(NA, c(N,N,no.draws))
  B0.aux        = B0.initial
  
for (i in 1:no.draws){
    for (n in 1:N){
      rn            = nrow(V[[n]])
      SS_tmp        = nu*solve(V[[n]]%*%S.inv%*%t(V[[n]]))
      SS_tmp        = 0.5 * (SS_tmp + t(SS_tmp))
      Un            = chol(SS_tmp)
      w             = t(orthogonal.complement.matrix.TW(t(B0.aux[-n,])))
      w1            = w %*% t(V[[n]]) %*% t(Un) / sqrt(as.numeric(w %*% t(V[[n]]) %*% t(Un) %*% Un %*% V[[n]] %*% t(w)))
      if (rn>1){
        Wn          = cbind(t(w1),orthogonal.complement.matrix.TW(t(w1)))
      } else {
        Wn          = w1
      }
      alpha         = rep(NA,rn)
      u             = rmvnorm(1,rep(0,nu+1),(1/nu)*diag(nu+1))
      alpha[1]      = sqrt(as.numeric(u%*%t(u)))
      if (runif(1)<0.5){
        alpha[1]    = -alpha[1]
      }
      if (rn>1){
        alpha[2:rn] = rmvnorm(1,rep(0,nrow(V[[n]])-1),(1/nu)*diag(rn-1))
      }
      bn            = alpha %*% Wn %*% Un
      B0.aux[n,]    = bn %*% V[[n]]
    }
    B0[,,i]         = B0.aux
  }
  
  return(B0)
}
```

The third function used for the algorithm normalizes the matrix of the
contemporaneous effects.

```{r thridfunc}
#| echo: true
#| message: false
#| warning: false
normalization.wz2003  = function(B0,B0.hat.inv, Sigma.inv, diag.signs){
  # B0        - an NxN matrix, to be normalized
  # B0.hat    - an NxN matrix, a normalized matrix
  
  N                 = nrow(B0)
  K                 = 2^N
  distance          = rep(NA,K)
  for (k in 1:K){
    B0.tmp.inv      = solve(diag(diag.signs[k,]) %*% B0)
    distance[k]     = sum(
      unlist(
        lapply(1:N,
               function(n){
                 t(B0.tmp.inv - B0.hat.inv)[n,] %*%Sigma.inv %*% t(B0.tmp.inv - B0.hat.inv)[n,]
               }
        )))
  }
  B0.out            = diag(diag.signs[which.min(distance),]) %*% B0
  
  return(B0.out)
}
```

The following function normalizes the output from the rgn function,
ensuring that we are in a unique equilibrium, as discussed above.

```{r fourthfunc}
#| echo: true
#| message: false
#| warning: false
normalize.Gibbs.output.parallel          = function(B0.posterior,B0.hat){
  # B0.posterior  - a list, output from function rgn
  # B0.hat        - an NxN matrix, a normalized matrix
  
  N                 = nrow(B0.hat)
  K                 = 2^N
  
  B0.hat.inv        = solve(B0.hat)
  Sigma.inv         = t(B0.hat)%*%B0.hat
  
  diag.signs        = matrix(NA,2^N,N)
  for (n in 1:N){
    diag.signs[,n]  = kronecker(c(-1,1),rep(1,2^(n-1)))
  }
  
  B0.posterior.n    = mclapply(1:dim(B0.posterior)[3],function(i){
    normalization.wz2003(B0=B0.posterior[,,i],B0.hat.inv, Sigma.inv, diag.signs)
  },mc.cores=1
  )
  B0.posterior.n  = simplify2array(B0.posterior.n)
  
  return(B0.posterior.n)
}
```

Lastly we need a function simulating the draws of the multivariate
normal distribution of the autoregressive slope matrix.

```{r fifthfunc}
#| echo: true
#| message: false
#| warning: false
rnorm.ngn       = function(B0.posterior,B,Omega){
  # B0.posterior  - a list, output from function rgn
  # B             - an NxK matrix, a parameter determining the mean of the multivariate conditionally normal distribution given B0
  # Omega         - a KxK positive definite matrix, a covariance matrix of the multivariate normal distribution
  
  N             = nrow(B)
  K             = ncol(B)
  no.draws      = dim(B0.posterior)[3]
  L             = t(chol(Omega))
  
  Bp.posterior  = lapply(1:no.draws,function(i){
    Bp          = matrix(NA, N, K)
    for (n in 1:N){
      Bp[n,]    = as.vector(t(B0.posterior[n,,i] %*% B) + L%*%rnorm(K))
    }
    return(Bp)
  })
  Bp.posterior  = simplify2array(Bp.posterior)
  return(Bp.posterior)
}
```

Now having introduced the functions applied throughout the Gibbs
sampler, the artificial data can be generated and the:

```{r AD}
#| echo: true
#| message: false
#| warning: false
T       = 500
N       = 3
p       = 1
K       = 1 + N * p

y.sim       = apply(matrix(rnorm(T * N), ncol = N), 2, cumsum)

# create Y and X
############################################################
Y.sim       = y.sim[(p+1):nrow(y.sim),]
X.sim       = matrix(1,nrow(Y.sim),1)
for (i in 1:p){
  X.sim     = cbind(X.sim,y.sim[((p+1):nrow(y.sim))-i,])
}
Y.sim       = t(Y.sim)
X.sim       = t(X.sim)
```

Before running the algorithm we need the set the priors according to the
baseline model, aforementioned, and impose the required exclusionary
restrictions, which in this model will be done by following a recursive
scheme.

```{r}
#| echo: true
#| message: false
#| warning: false

# set the priors
kappa1  = .1       # Autoregressive slope shrinkage
kappa2  = 10       # Constant term shrinkage
kappa0  = 10       # Contemporaneous effects shrinkage

priors  = list(
  B     = cbind(rep(0,N), diag(N), matrix(0, N, (p-1)*N)),
  Omega = diag(c(kappa2,kappa1*((1:p)^(-2))%x%rep(1,N))),
  # Omega = diag(c(kappa2,kappa1*rep(1,N*p))),
  S     = kappa0*diag(N),
  nu    = N
)

#Exclusions (can be changed to different exclusions then cholesky) 
FF.V           = vector("list",N)
for (n in 1:N){
  FF.V[[n]]   = cbind(diag(n),matrix(0,n,N-n))
}

# The B0.initial is used as an initial matrix used in the Gibbs sampler
B0.initial = matrix(0,N,N)
for (n in 1:N){
  unrestricted    = apply(FF.V[[n]],2,sum)==1
  B0.initial[n,unrestricted] = rnorm(sum(unrestricted))
}
```

The function for Gibbs sampler of the baseline model can be found below:

```{r}
#| echo: true
#| message: false
#| warning: false

## Gibbs sampler for posterior simulations ##
Gibbs.sampler.base <- function(p,Y,X,priors,S1,S2, FF.V, B0.initial){

  N       = nrow(Y)
  p       = p # calculate from X and Y (K and N)
  K       = 1+N*p
  S1      = S1
  S2      = S2
  kappa0 = 10
  kappa1 = 10
  kappa2 = 0.1

  B0.posterior    <- array(NA,c(N,N,(S1+S2)))
  Bp.posterior    <- array(NA,c(N,(1+N*p),(S1+S2)))

  for (s in 1:(S1+S2)){

    # Computing posterior parameters
    Omega.inv      = solve(priors$Omega)
    Omega.post.inv = X%*%t(X) + Omega.inv
    Omega.post     = solve(Omega.post.inv)
    B.post         = (Y%*%t(X) + priors$B%*%Omega.inv) %*% Omega.post
    S.post         = Y%*%t(Y) + solve(priors$S) + priors$B%*%Omega.inv%*%t(priors$B) - B.post%*%Omega.post.inv%*%t(B.post) 
    nu.post        = ncol(Y) + priors$nu

    # Use B0.initial for first iteration, otherwise the latest draw from B0.posterior
    if (s==1) {
      B0.s = B0.initial
    } else {
      B0.s = B0.posterior[,,s-1]
    }

    # sampling one draw B0 from the posterior distribution using Gibbs
    # rgn.function samples from a random conditional generalized normal distribution
    B0.tmp                  = rgn(n=1, S.inv=S.post, nu=nu.post, V=FF.V, B0.initial=B0.s)
    B0.posterior[,,s]       = B0.tmp[,,1]

    # sample one draw B+ from the normal conditional posterior
    Bp.tmp              = rnorm.ngn(B0.tmp, B=B.post,Omega=Omega.post)
    Bp.posterior[,,s]   = Bp.tmp[,,1]
  }
  # END OF GIBBS
  #Discard first S1 draws
  B0.posterior <- B0.posterior[,,(S1+1):(S1+S2)]
  Bp.posterior <- Bp.posterior[,,(S1+1):(S1+S2)]

  #normalisation of B0.posterior and Bp.posterior
  B0.hat             = diag(sign(diag(B0.tmp[,,1]))) %*% B0.tmp[,,1]

  B0.posterior.N    <- array(NA,c(N,N,S2))
  Bp.posterior.N    <- array(NA,c(N,(1+N*p),S2))

  B0.posteror.N.tmp      =  normalize.Gibbs.output.parallel(B0.posterior,B0.hat=B0.hat)
  for (s in 1:S2){
    B0.posterior.N[,,s]    = B0.posteror.N.tmp[,,s]
    Bp.posterior.N[,,s]    = B0.posterior.N[,,s]%*%solve(B0.posterior[,,s])%*%Bp.posterior[,,s]
  }

  return(list(B0.posterior.N = B0.posterior.N,
              Bp.posterior.N = Bp.posterior.N))
}
```

```{r baseline show}
#| echo: false
#| message: false
#| warning: false
# Run Basic function
Basics = Gibbs.sampler.base(p=1,Y=Y.sim,X=X.sim,priors=priors,S1=200,S2=1800, FF.V=FF.V, B0.initial=B0.initial)
"B_0"
round(apply(Basics$B0.posterior.N, 1:2, mean),3)
"B_+"
round(apply(Basics$Bp.posterior.N, 1:2, mean),3)
```

The results indicates that the exclusion restrictions, which in this
case is modelled by a recursive structure is implemented on the
structural matrix,$B_0$, as illustrated in the former section. We see
that the posterior mean of the constant term, which is found in the
first column of matrix $B_+$ is close to zero, as well as the remaining
part can be identified as an identity matrix.

## Extended model

In order to improve the estimation of the parameters, it is standard in
bayesian econometrics to estimate the hyper-parameters contrary to
setting them to fixed values as in the case of the basic model.
Estimating the hyper-parameters is often done to improve the model, and
is demonstrated to have a lot of power in terms of the overall
likelihood of the model, as demonstrated in @chan2022asymmetric.

The extension proposed in this research paper is therefore to estimate
$\kappa_0$ and $\kappa_+$, where $\kappa_+$ contains the shrinkage of
the constant term as well as the shrinkage of the slope of the
autoregressive parameters, and $\kappa_0$ is the shrinkage of the
structural matrix.

In the extended model, the natural-conjugate prior is given by, where we
note, that the hyper-parameters follows an Inverse-gamma-2 distribution:

```{=tex}
\begin{align}
p(B_+,B_0|Y,X) &\propto L(B_+,B_0|Y,X)p(B_+,B_0|\kappa_0,\kappa_+)p(\kappa_0)p(\kappa_+)\\
\end{align}
```
```{=tex}
\begin{align}
p(\kappa_0|\underline{s}_{\kappa_0},\underline{\nu}_{\kappa_0}) &\sim \mathcal{IG}2(\underline{s}_{\kappa_0},\underline{\nu}_{\kappa_0})\\
p(\kappa_+|\underline{s}_{\kappa_+},\underline{\nu}_{\kappa_+}) &\sim \mathcal{IG}2(\underline{s}_{\kappa_+},\underline{\nu}_{\kappa_+})
\end{align}
```
Now finding an expression for the Full-conditional posterior of
$\kappa_0$, we can write this as:

```{=tex}
\begin{align}
p(\kappa_0|Y,X,B_0,B_+,\kappa_+) &\propto p(B_0|\kappa_0)p(\kappa_0)\\
&\propto \prod_{n=1}^N\kappa_0^{\frac{r_n}{2}}\exp \left\{  -\frac{1}{2}\sum_{n=1}^N b_nV_n(\kappa_0 I_{r_n})^{-1}V_n'b_n'\right\}\kappa_0^{-\frac{\underline{\nu}_{\kappa_0}+2}{2}}\exp \left\{  -\frac{1}{2} \frac{\underline{s}_{\kappa_0}}{\kappa_0}\right\}\\
&\propto \prod_{n=1}^N\kappa_0^{\frac{r_n}{2}} \exp \left\{  -\frac{1}{2}\frac{1}{\kappa_0}\sum_{n=1}^N b_nV_n I_{r_n}V_n'b_n'\right\}\kappa_0^{-\frac{\underline{\nu}_{\kappa_0}+2}{2}}\exp \left\{  -\frac{1}{2} \frac{\underline{s}_{\kappa_0}}{\kappa_0}\right\}
\end{align}
```
where we have used that that $\underline{S}=\kappa_0I_N$ and that
$b_n|\kappa_0 \sim \mathcal{N}(0,\kappa_0(V_nV_n')^{-1})=\mathcal{N}_{r_n}(0_{r_n},\kappa_0I_{r_n})$.
Thus by collecting the terms accordingly, we are able to determine the
full conditional posterior, given by the shape parameter,
$\bar{S_{\kappa_0}}$, and the degrees of freedom,
$\bar{\nu}_{\kappa_0}$:

```{=tex}
\begin{align}
p(\kappa_0|Y,X,B_0,B_+,\kappa_+) &\propto \kappa_0^{-\frac{\bar{\nu}_{\kappa_0}+2}{2}} \exp \left\{ -\frac{1}{2}\frac{\bar{s}_{\kappa_0}}{\kappa_0} \right\}\\
\bar{s}_{\kappa_0} &= \underline{s}_{\kappa_0}+\sum_{n=1}^N b_nV_nI_{r_n}V_n'b_n'\\
\bar{\nu}_{\kappa_0} &= \underline{\nu}_{\kappa_0}+\sum_{n=1}^N r_n
\end{align}
```
Doing the same excercise for $\kappa_+$ gives us and expression for the
full-conditional posterior:

```{=tex}
\begin{align}
p(\kappa_+|Y,X,B_0,B_+,\kappa_0) &\propto p(B_+|B_0,\kappa_+)p(\kappa_+)\\
&\propto \kappa_+^{\frac{K}{2}}\exp \left\{-\frac{1}{2}\frac{1}{\kappa_+} \sum_{n=1}^N (B_n-b_nV_n\underline{B})\underline{\Omega}^{-1}(B_n-b_nV_n\underline{B})'\right\}\kappa_+^{-\frac{\underline{\nu}_{\kappa_+}+2}{2}}\exp \left\{  -\frac{1}{2} \frac{\underline{s}_{\kappa_+}}{\kappa_+}\right\}
\end{align}
```
where we have used that
$B_n|b_n,\kappa_+ \sim \mathcal{N}_{N+1}(b_nV_n\underline{B},\kappa_+\Omega)$

Following the aforementioned arguments, the posterior parameters can be
expressed as:

```{=tex}
\begin{align}
p(\kappa_+|Y,X,B_0,B_+,\kappa_0) &\propto \kappa_+^{-\frac{\bar{\nu}_{\kappa_+}+2}{2}} \exp \left\{ -\frac{1}{2}\frac{\bar{s}_{\kappa_+}}{\kappa_+} \right\}\\
\bar{s}_{\kappa_+} &= \underline{s}_{\kappa_+}+\sum_{n=1}^N (B_n-b_nV_n\underline{B})\underline{\Omega}^{-1}(B_n-b_nV_n\underline{B})'\\
\bar{\nu}_{\kappa_+} &= \underline{\nu}_{\kappa_+}+NK
\end{align}
```
Turning to the implementation of the extension we can by following the
derivations outlined above write the priors of the extended model as
below

```{r}
#| echo: true
#| message: false
#| warning: false

### Setting new priors
priors   = list(
  B        = cbind(rep(0,N), diag(N), matrix(0, N, (p-1)*N)),
  Omega    = diag(c(10,((1:p)^(-2))%x%rep(1,N))),
  S        = diag(N),
  nu       = N,
  S.kappa0  = 1,
  nu.kappa0 = 1,
  S.kappa1  = 1,
  nu.kappa1 = 1
)

```

The function for Gibbs sampler of the extended model can be found below:

```{r}
#| echo: true
#| message: false
#| warning: false


# The B0.initial is used as an initial matrix used in the Gibbs sampler
B0.initial = matrix(0,N,N)
for (n in 1:N){
  unrestricted    = apply(FF.V[[n]],2,sum)==1
  B0.initial[n,unrestricted] = rnorm(sum(unrestricted))
}


## Gibbs sampler for posterior simulations ##
Gibbs.sampler.extended <- function(p,Y,X,priors,S1,S2, FF.V, B0.initial){
  
  N       = nrow(Y)
  p       = p # calculate from X and Y (K and N)
  K       = 1+N*p
  S1      = S1
  S2      = S2
  
  kappa0          <- rep(NA, S1 + S2)
  kappa1          <- rep(NA, S1 + S2)
  B0.posterior    <- array(NA,c(N,N,(S1+S2)))
  Bp.posterior    <- array(NA,c(N,(1+N*p),(S1+S2)))  
  
  kappa0[1] <- 1
  kappa1[1] <- 1 
  
  for (s in 1:(S1+S2)){
    
    # Computing posterior parameters
    Omega.inv      = solve(priors$Omega)
    Omega.post.inv = X%*%t(X) + (1/kappa1[s])*Omega.inv
    Omega.post     = solve(Omega.post.inv)
    B.post         = (Y%*%t(X) + priors$B%*%((1/kappa1[s])*Omega.inv)) %*% Omega.post
    S.post         = Y%*%t(Y) + (1/kappa0[s])*solve(priors$S) + priors$B%*%((1/kappa1[s])*Omega.inv)%*%t(priors$B) - B.post%*%Omega.post.inv%*%t(B.post) 
    nu.post        = ncol(Y) + priors$nu
    
    # Use B0.initial for first iteration, otherwise the latest draw from B0.posterior
    
    if (s==1) {
      B0.s = B0.initial
    } else {
      B0.s = B0.posterior[,,s-1]
    }
    
    # sampling one draw B0 from the posterior distribution using Gibbs  
    # rgn.function samples from a random conditional generalized normal distribution
    
    B0.tmp                  = rgn(n=1, S.inv=S.post, nu=nu.post, V=FF.V, B0.initial=B0.s)
    B0.posterior[,,s]       = B0.tmp[,,1]
    
    # sample one draw B+ from the normal conditional posterior
    Bp.tmp              = rnorm.ngn(B0.tmp, B=B.post,Omega=Omega.post)
    Bp.posterior[,,s]   = Bp.tmp[,,1]
    
    #compute posterior for the shrinkage parameter S.kappa and nu
    S.kappa0.post = priors$S.kappa0 + sum(B0.posterior[,,s]^2)
    
    # S.kappa.post = sum(priors$S.kappa + (B0.posterior[i,,s]-priors$B[i,])%*%Omega.inv%*%t(B0.posterior[i,,s]-priors$B[i,]))
    
    # nu.kappa0.post  = priors$nu.kappa0 + i #change outside of loop count number rows (otherwise make as a sum of i's)
    nu.kappa0.post  = priors$nu.kappa0 + sum(unlist(lapply(FF.V, nrow)))
    
    S.kappa1.post   = priors$S.kappa1
    for (i in 1:N){
      S.kappa1.post = S.kappa1.post + (Bp.posterior[i,,s]- B0.posterior[i,,s]%*%priors$B)%*%Omega.inv%*%t(Bp.posterior[i,,s]-B0.posterior[i,,s]%*%priors$B)
    }
    # S.kappa.post = sum(priors$S.kappa + (B0.posterior[i,,s]-priors$B[i,])%*%Omega.inv%*%t(B0.posterior[i,,s]-priors$B[i,]))
    
    nu.kappa1.post  = priors$nu.kappa1 + N*(p*N+1) 
    
    
    #Draw kappa0 and kappa1 from IG2
    if (s != S1+S2) {
      kappa0[s+1]    = S.kappa0.post / rchisq(1, df=nu.kappa0.post) 
      kappa1[s+1]    = S.kappa1.post / rchisq(1, df=nu.kappa1.post) 
    }
  }
  
  #Discard first S1 draws
  
  B0.posterior <- B0.posterior[,,(S1+1):(S1+S2)]
  Bp.posterior <- Bp.posterior[,,(S1+1):(S1+S2)]
  kappa0       <- kappa0[(S1+1):(S1+S2)]
  kappa1       <- kappa1[(S1+1):(S1+S2)]
  
  #normalisation of B0.posterior and Bp.posterior
  
  B0.hat             = diag(sign(diag(B0.tmp[,,1]))) %*% B0.tmp[,,1]
  # t(chol((nu.post-N)*S.post))# normalisation using this B0.hat should work
  
  B0.posterior.N    <- array(NA,c(N,N,S2))
  Bp.posterior.N    <- array(NA,c(N,(1+N*p),S2))
  
    B0.posteror.N.tmp      =  normalize.Gibbs.output.parallel(B0.posterior,B0.hat=B0.hat)
  for (s in 1:S2){
    B0.posterior.N[,,s]    = B0.posteror.N.tmp[,,1]
    Bp.posterior.N[,,s]    = B0.posterior.N[,,s]%*%solve(B0.posterior[,,s])%*%Bp.posterior[,,s]
  }
  
  return(list(B0.posterior.N = B0.posterior.N,
              Bp.posterior.N = Bp.posterior.N,
              kappa0 = kappa0,
              kappa1 = kappa1))
}
```

```{r extended show}
#| echo: false
#| message: false
#| warning: false
# Run Basic function
extended = Gibbs.sampler.extended(p=1,Y=Y.sim,X=X.sim,priors=priors,S1=200,S2=1800, FF.V=FF.V, B0.initial=B0.initial)
```

```{r extended dontshow}
#| echo: false
#| message: false
#| warning: false
# Run extended function
"B_0"
round(apply(extended$B0.posterior.N, 1:2, mean),3)
"B_+"
round(apply(extended$Bp.posterior.N, 1:2, mean),3)
"Kappa_0"
mean(extended$kappa0)
"Kappa_+"
mean(extended$kappa1)
```

As for the basic model, the parameters are estimated according to the
simulated data, with the exclusion restrictions imposed and an identity
matrix in $B_+$, as well as the mean of the constant term is zero. The
estimation of the hyper parameters are in general in line with the
literature, where the hyper parameter of the shrinkage of the structural
matrix, $\kappa_0$ is generally estimated to be larger, compared to
$\kappa_+$ which is relatively small. Additionally, in order to check
the convergence of the model, the estimated parameters in the diagonal
of the matrix $B_+$ is plotted against the number of draws. We see that
the convergence is complete, as the graph should can be considered white
noise. The estimations are computed using 2000 draws, where 200 of the
first observations are discarded. The low number of draws is due to the computational power required. 

```{r showing converge plot}
#| echo: false
#| message: false
#| warning: false
# Plotting convergence
par(mfrow=c(3,1))
plot(extended$Bp.posterior.N[,2,][1,],type='l',col="#660099",ylab="",xlab="",main=expression(B[+12]), lwd = 0.1)
plot(extended$Bp.posterior.N[,3,][2,],type='l',col="#CC66CC",ylab="",xlab="",main=expression(B[+23]), lwd = 0.1)
plot(extended$Bp.posterior.N[,4,][3,],type='l',col="#CC66CC",ylab="",xlab="",main=expression(B[+34]), lwd = 0.1)

```

## Empirical results

**Identification**

In order for us to do the impulse responses, we first have to implement
the exclusionary restricitions according to the identification of the
desired shock. Starting from the structural matrix presented in the
sections above. The recursive scheme is implemented as presented below:

```{=tex}
\begin{align}
        B_0 Y=\begin{bmatrix} b_{11} & 0&0&0&0&0&0 \\
    b_{21} & b_{22} & 0&0&0&0&0 \\
    b_{31} & b_{32} & b_{33} &0&0&0 &0 \\
    b_{41} & b_{42} & b_{43} &b_{44}&0&0&0  \\
    b_{51} & b_{52} & b_{53} &b_{54}&b_{55}&0&0 \\
    b_{61} & b_{62} & b_{63} &b_{64}&b_{65}&b_{66}&0\\
        b_{71} & b_{72} & b_{73} &b_{74}&b_{75}&b_{76}&b_{77}\\
    \end{bmatrix}\begin{bmatrix} IP \\ CPI \\ FF \\ Exp \\ Lend \\ HP \\ NFCI \end{bmatrix}
\end{align}
```
The ordering follows the arguments of @JensenandRoager2019, where
financial variables, which can be considered fast moving are ordered
before the variables representing the real economy. One should note that
the Federal funds rate has been placed almost in the middle of the
system. The placement is supposed to act as a standard Taylor rule,
where we see, that the central bank adjusts its rates according to
movements in prices and output, which are not allowed to affect these
variables contemporaneously, according to an outside lag of monetary
policy transmission of one month. We see from the structural matrix,
that NFCI is placed last, which implies that the variable it self are
allowed to respond to shocks from the other variables contemporaneously,
however, a shock from financial conditions will only have an effect on
the remaining variables in the subsequent month, this seems reasonable,
as one might expect the other variables such as expectations, rates and
house prices might need some time to adjust according to the new
information.

**Impulse Response Functions**

Starting from a general VAR we know that we can write it in its Vector
Moving Average respresentation (VMA) assuming that the stationarity conditions is fulfilled. Furthermore, introducing the
matrix $J=\left[I_n\quad 0_{N\times N(p-1)}\right]$, and that we
consider a structural model, the model can expressed as follows:

```{=tex}
\begin{align}
y_t &=Bu_t+J\textbf{A}J'Bu_{t-1}+J\textbf{A}^2J'Bu_{t-2}+...\\
&=\Theta_0u_t+\Theta_1u_{t-1}+\Theta_2u_{t-2}+...\\
\end{align}
```
where $\frac{\partial y_{t+i}}{\partial u_t}=\Theta_i=\theta_iB$ is the IRF. The
IRFs are computed using the following function:

```{r computing IRFs}
irf <- function(p,B0.posterior,Bp.posterior,h,shock){
  
#variable number for shock. 
np = shock  
h = h  
p = p
N = dim(B0.posterior)[2]
S2 <- dim(B0.posterior)[3]
K  <- 1+N*p
  
mcxs1  = "#05386B"
mcxs2  = "#379683"
mcxs3  = "#5CDB95"
mcxs4  = "#8EE4AF"
mcxs5  = "#EDF5E1"
purple = "#b02442"
            
mcxs1.rgb   = col2rgb(mcxs1)
mcxs1.shade1= rgb(mcxs1.rgb[1],mcxs1.rgb[2],mcxs1.rgb[3], alpha=120, maxColorValue=255)
mcxs2.rgb   = col2rgb(mcxs2)
mcxs2.shade1= rgb(mcxs2.rgb[1],mcxs2.rgb[2],mcxs2.rgb[3], alpha=120, maxColorValue=255)


# Impulse response functions
# Forecast Error Variance Decomposition
############################################################
B.posterior       = array(NA,c(N,N,S2))
A.posterior       = array(NA,c(N,K,S2))
for (s in 1:S2){
  B               = solve(B0.posterior[,,s])
  B.posterior[,,s]= B
  A.posterior[,,s]= B %*% Bp.posterior[,,s]
}

IRF.posterior     = array(NA,c(N,N,h+1,S2))
IRF.inf.posterior = array(NA,c(N,N,S2))
FEVD.posterior    = array(NA,c(N,N,h+1,S2))
J                 = cbind(diag(N),matrix(0,N,N*(p-1)))
for (s in 1:S2){
  A.bold          = rbind(A.posterior[,2:(1+N*p),s],cbind(diag(N*(p-1)),matrix(0,N*(p-1),N)))
  IRF.inf.posterior[,,s]          = J %*% solve(diag(N*p)-A.bold) %*% t(J) %*% B.posterior[,,s]
  A.bold.power    = A.bold
  for (i in 1:(h+1)){
    if (i==1){
      IRF.posterior[,,i,s]        = B.posterior[,,s]
    } else {
      IRF.posterior[,,i,s]        = J %*% A.bold.power %*% t(J) %*% B.posterior[,,s]
      A.bold.power                = A.bold.power %*% A.bold
    }
  }
}
FEVD.posterior    = 100*FEVD.posterior

# plot IRFs and FEVDs
############################################################
IRF.posterior.mps = IRF.posterior[,np,,]
IRFs.k1           = apply(IRF.posterior.mps,1:2,median)
IRFs.k1           = apply(IRF.posterior.mps,1:2,median)
IRFs.inf.k1       = apply(IRF.posterior.mps,1,mean)
rownames(IRFs.k1) = colnames(y)

IRFs.k1.hdi    = apply(IRF.posterior.mps,1:2,hdi, credMass=0.68)
hh          = 1:(h+1)

pl = par(mfrow=c(4,2), mar=c(4,4.5,2,2),cex.axis=1.5, cex.lab=1.5)
for (n in 1:N){
  ylims     = range(IRFs.k1[n,hh],IRFs.k1.hdi[,n,1:6],0)
  plot(hh,IRFs.k1[n,hh], type="l", ylim=ylims, axes=FALSE, xlab="", ylab=rownames(IRFs.k1)[n])
  if (n==5 | n==6){
    axis(1,c(1,13,25,37),c("1 Month","1 year","2 years","3 years"))
  } else {
    axis(1,c(1,13,25,37),c("1 Month","1 year","2 years","3 years"))
  }
  axis(2,c(ylims[1],0,ylims[2]),round(c(ylims[1],0,ylims[2]),3))
  polygon(c(hh,(h+1):1), c(IRFs.k1.hdi[1,n,hh],IRFs.k1.hdi[2,n,(h+1):1]), col=mcxs2.shade1,border=mcxs2.shade1)
  abline(h=0)
  lines(hh, IRFs.k1[n,hh],lwd=2,col=mcxs1)
}
}

```

```{r run both models with data}
#| echo: false
#| message: false
#| warning: false
#change data 
y = na.omit(merge(tindp, tcpi,tFF, texpec, tlend, thp , tfci))
colnames(y)<- c("IP", "CPI","FF", "EXP", "LEND","HP","NFCI")
y = as.ts(y)

p=12
Y       = y[(p+1):nrow(y),]
X       = matrix(1,nrow(Y),1)
for (i in 1:p){
  X     = cbind(X,y[((p+1):nrow(y))-i,])
}

N       = ncol(Y)
K       = 1+N*p

Y       = t(Y)
X       = t(X)


## setting exclusions and restrictions
#Exclusions (can be changed to different exclusions then cholesky)
FF.V           = vector("list",N)
for (n in 1:N){
  FF.V[[n]]   = cbind(diag(n),matrix(0,n,N-n))
}

# The B0.initial is used as an initial matrix used in the Gibbs sampler
B0.initial = matrix(0,N,N)
for (n in 1:N){
  unrestricted    = apply(FF.V[[n]],2,sum)==1
  B0.initial[n,unrestricted] = rnorm(sum(unrestricted))
}

# set the priors for basic model
kappa1  = .1       # Autoregressive slope shrinkage
kappa2  = 10       # Constant term shrinkage
kappa0  = 10       # Contemporaneous effects shrinkage

priors  = list(
  B     = cbind(rep(0,N), diag(N), matrix(0, N, (p-1)*N)),
  Omega = diag(c(kappa2,kappa1*((1:p)^(-2))%x%rep(1,N))),
  # Omega = diag(c(kappa2,kappa1*rep(1,N*p))),
  S     = kappa0*diag(N),
  nu    = N
)

base  = Gibbs.sampler.base(p=12,Y=Y,X=X,priors=priors, S1=200,S2=1800, FF.V=FF.V, B0.initial=B0.initial)

# setting priors for extended
priors    = list(
  B         = cbind(rep(0,N), diag(N), matrix(0, N, (p-1)*N)),
  Omega     = diag(c(10,((1:p)^(-2))%x%rep(1,N))),
  S         = diag(N),
  nu        = N,
  S.kappa0  = 1,
  nu.kappa0 = 1,
  S.kappa1  = 1,
  nu.kappa1 = 1
)

FF.V           = vector("list",N)
for (n in 1:N){
  FF.V[[n]]   = cbind(diag(n),matrix(0,n,N-n))
}

# The B0.initial is used as an initial matrix used in the Gibbs sampler
B0.initial = matrix(0,N,N)
for (n in 1:N){
  unrestricted    = apply(FF.V[[n]],2,sum)==1
  B0.initial[n,unrestricted] = rnorm(sum(unrestricted))
}

exten = Gibbs.sampler.extended(p=12,Y=Y,X=X,priors=priors,S1=200,S2=1800, FF.V=FF.V, B0.initial=B0.initial)
```

## Basic model

The impulse responses for the baseline and extended model are presented
with confidence bands of one standard deviation being equivalent to the
68 pct. level of significance. The plots can be found below. 

```{r show IRFs basic}
#| echo: false
#| message: false
#| warning: false
#| label: fig-irf-basic-plot
#| fig-cap: "IRFs for basic model" 

irf(B0.posterior=base$B0.posterior.N,Bp.posterior=base$Bp.posterior.N,p=p,h=36,shock = 7)

```

## Extended model

```{r show IRFs extended}
#| echo: false
#| message: false
#| warning: false
#| label: fig-irf-extended-plot
#| fig-cap: "IRFs for extended model" 

irf(B0.posterior=exten$B0.posterior.N,Bp.posterior=exten$Bp.posterior.N,p=p,h=36,shock = 7)
```

We see that following a positive shock to financial conditions, which is
equivalent to a tightening of financial conditions, the real
economic variables responds according to the hypothesis presented above.
As Financial conditions tighten we see that industrial production as
well as consumer prices decrease. This is what one would expect
according to economic theory, as the tighter conditions of finance
should make it harder for firms and consumers to acquire liquidity for
investments and consumption smoothing: Another possible channel is that
as financial conditions are tightened consumers expect that the economic
situation will be worse compared to the current state and thus starts
increasing their savings, implying a lower demand for goods, affecting
the industrial production as well as the consumer prices negatively.
Following the observed movements in the real economic variables the
central bank responds by lowering the federal funds rate in order to
stimulate the economy.

Looking at the financial variables we see that the house prices are
decreasing, which is explained by the tougher financial conditions.
However, rather surprisingly we see that the initial response of lending
is positive, after which it decreases substantially. The initial
response is not what you would expect, as the tighter conditions in the
financial markets should impact the liquidity market negatively and thus
decrease the total lending. Nevertheless, this might simply be explained
by the fact that there is a lag from the tightening until an actual
implementation. All the responses are significant.

From the plots of the basic and the extended model it is evident, that the overall picture is very similar,
however, what we gain from estimating the hyper parameters is that the
confidence bands are significantly narrowed, which implies that we have
essentially reduced de uncertainty in the model, thus giving us more
reliable responses.

## Robustness

When applying a recursive structure, it is standard in the literature to
provide a robustness check, testing whether the empirical results change
as the structure is changes. Thus, in order to comply with this i
consider a model where the shock of interest has been placed first, and
thus
$Y=\begin{bmatrix}NFCI & IP & CPI & FF & Exp & Lend & HP \end{bmatrix}'$.
This implies that there is contemporaenous effects from a shock in
financial coditions onto the real economy. The IRFs can be found below.

```{r run both models with data 2}
#| echo: false
#| message: false
#| warning: false
#change data 
y = na.omit(merge(tfci, tindp, tcpi, tFF, texpec, tlend, thp))
colnames(y)<- c("NFCI","IP", "CPI","FF", "EXP", "LEND","HP")
y = as.ts(y)

p=12
Y       = y[(p+1):nrow(y),]
X       = matrix(1,nrow(Y),1)
for (i in 1:p){
  X     = cbind(X,y[((p+1):nrow(y))-i,])
}

N       = ncol(Y)
K       = 1+N*p

Y       = t(Y)
X       = t(X)


# setting priors for extended
priors   = list(
  B        = cbind(rep(0,N), diag(N), matrix(0, N, (p-1)*N)),
  Omega    = diag(c(10,((1:p)^(-2))%x%rep(1,N))),
  S        = diag(N),
  nu       = N,
  S.kappa0  = 1,
  nu.kappa0 = 1,
  S.kappa1  = 1,
  nu.kappa1 = 1
)

FF.V           = vector("list",N)
for (n in 1:N){
  FF.V[[n]]   = cbind(diag(n),matrix(0,n,N-n))
}

# The B0.initial is used as an initial matrix used in the Gibbs sampler
B0.initial = matrix(0,N,N)
for (n in 1:N){
  unrestricted    = apply(FF.V[[n]],2,sum)==1
  B0.initial[n,unrestricted] = rnorm(sum(unrestricted))
}

extenrob = Gibbs.sampler.extended(p=12,Y=Y,X=X,priors=priors,S1=200,S2=1800, FF.V=FF.V, B0.initial=B0.initial)

```

```{r show IRFs extendedrob}
#| echo: false
#| message: false
#| warning: false
#| label: fig-irf-extended-plot-rob
#| fig-cap: "IRFs Robustness" 

irf(B0.posterior=extenrob$B0.posterior.N,Bp.posterior=extenrob$Bp.posterior.N,p=p,h=36,shock = 1)

```

From the plot it is evident that the long run response of CPI and industrial production is similar to the prior IRFs, however, we see an initial increase, which is in contrast to what economic theory would suggest. Looking at the response of the Federal reserve, we see that the interest rate is now increased. This is different from the prior IRFs where they indicated a reduction. Looking at the consumer expectations we see that they increase, which might be explained by the fact that agents are assuming that tightening stems from an indication of a bright future ahead. The response of house prices and lending are in line with the previous plots.

The different responses of the Federal Reserve might be explained by two possible channels. In the first ordering, the response would be considered a help to the economy, after seeing the economic downturn forced by the tighter financial conditions. In the robust setting, the response would indicate, that the interest rate on treasury bonds, which closely follows the federal funds rate included in the NFCI, has increased. 

**Summing up the empirical investigation**

Summing up, the hypothesis, stating that financial conditions affects
the real economy cannot be rejected, as we see that it indeed has a
significant effect, and it is thus very important to policy makers to
take this into account when doing financial regulation and/or monetary
policy. Furthermore, changing the order of the variables does not have a
significant effect on the empirical results regarding the real economic variables. Nevertheless, it does have a significant effect on the response of the Federal Reserve as well as the consumer expectations.

# References {.unnumbered}
